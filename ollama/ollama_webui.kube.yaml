---
apiVersion: v1
kind: Pod
metadata:
  annotations:
    io.containers.autoupdate/webui: registry
    io.containers.autoupdate/ollama: local
    io.podman.annotations.privileged: "FALSE"
    io.podman.annotations.autoremove: "TRUE"
  labels:
    app: ollama-pod
  name: ollama-pod
spec:
  containers:
  - name: ollama
    env:
      - name: OLLAMA_MODELS
        value: /models
      - name: OLLAMA_GPU_OVERHEAD
        value: 3300000000 # reverse 3GB for desktop usage
      - name: OLLAMA_INTEL_GPU
        value: 1
    image: localhost/ollama:vulkan
    imagePullPolicy: Never
    command: ["ollama"]
    args: ["serve"]
    volumeMounts:
      - mountPath: /models
        name: ollama-models-pvc
      - mountPath: /root/.ollama
        name: ollama-data-pvc
      - mountPath: /dev/dri
        name: dri-devices
    # --security-opt label=disable
    securityContext:
      seLinuxOptions:
        type: spc_t
    # --device 
    resources:
      limits:
        nvidia.com/gpu=all: 1

  - name: webui
    image: ghcr.io/open-webui/open-webui:main
    imagePullPolicy: IfNotPresent
    env:
      - name: OLLAMA_BASE_URL
        value: http://localhost:11434
    ports:
      - containerPort: 8080
        hostPort: 8077
    volumeMounts:
      - mountPath: /app/backend/data
        name: open-webui-pvc
  volumes:
    - name: open-webui-pvc
      persistentVolumeClaim:
        claimName: open-webui
    - name: ollama-models-pvc
      persistentVolumeClaim:
        claimName: ollama-models
    - name: ollama-data-pvc
      persistentVolumeClaim:
        claimName: ollama-data
    - name: dri-devices
      hostPath:
        path: /dev/dri
        type: Directory
