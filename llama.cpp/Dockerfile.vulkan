FROM docker.io/ubuntu:25.04 AS build

RUN --mount=type=cache,target=/var/cache/apt \
    apt-get update && apt-get install -y --no-install-suggests \
    git \
    g++ \
    cmake \
    ninja-build \
    libvulkan-dev \
    vulkan-tools \
    libcurl4-openssl-dev \
    glslc

WORKDIR /workspace/llama.cpp
RUN --mount=type=tmpfs,target=/workspace \
    git clone https://github.com/ggerganov/llama.cpp.git /workspace/llama.cpp && \
    cmake -B build \
    -DCMAKE_INSTALL_PREFIX=/opt/llama.cpp \
    -GNinja \
    -DLLAMA_BUILD_TESTS=OFF \
    -DGGML_LTO=ON \
    -DGGML_AVX=ON \
    -DGGML_AVX2=ON \
    -DGGML_VULKAN=ON && \
    cmake --build build && \
    cmake --install build

FROM docker.io/debian:sid
ENV PATH="/opt/llama.cpp/bin:${PATH}"
ENV LD_LIBRARY_PATH="/opt/llama.cpp/lib"


COPY --from=build /opt/llama.cpp /opt/llama.cpp

# libxext6 for libGLX_nvidia.so.0
# libegl1 for nvidia vulkan icd
# llama.cpp built with libcurl
RUN --mount=type=cache,target=/var/cache/apt \
    apt-get update && \
    apt-get install -y libvulkan1 libgomp1 curl vulkan-tools libegl1 libxext6 && \
    rm -rf /var/lib/apt/lists/*

RUN mkdir -p /usr/share/glvnd/egl_vendor.d && \
    cat > /usr/share/glvnd/egl_vendor.d/10_nvidia.json <<EOF
{
    "file_format_version" : "1.0.0",
    "ICD" : {
        "library_path" : "libEGL_nvidia.so.0"
    }
}
EOF

ENV NVIDIA_VISIBLE_DEVICES=all 
ENV NVIDIA_DRIVER_CAPABILITIES=all

ENTRYPOINT []
