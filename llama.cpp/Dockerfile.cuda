FROM docker.io/nvidia/cuda:13.0.1-cudnn-devel-ubuntu24.04 AS build

RUN --mount=type=cache,target=/var/cache/apt \
    apt-get update && apt-get install -y --no-install-suggests \
    git \
    g++ \
    cmake \
    ninja-build \
    libvulkan-dev \
    vulkan-tools \
    libcurl4-openssl-dev \
    glslc

WORKDIR /workspace/llama.cpp
RUN --mount=type=tmpfs,target=/workspace \
    git clone https://github.com/ggerganov/llama.cpp.git /workspace/llama.cpp && \
    cmake -B build \
    -DCMAKE_INSTALL_PREFIX=/opt/llama.cpp \
    -GNinja \
    -DLLAMA_BUILD_TESTS=OFF \
    -DGGML_LTO=ON \
    -DGGML_AVX=ON \
    -DGGML_AVX2=ON \
    -DGGML_CUDA=ON \
    -DGGML_VULKAN=ON \
    && \
    cmake --build build && \
    cmake --install build

FROM docker.io/nvidia/cuda:13.0.1-cudnn-runtime-ubuntu24.04
ENV PATH="/opt/llama.cpp/bin:${PATH}"
ENV LD_LIBRARY_PATH="/opt/llama.cpp/lib"

COPY --from=build /opt/llama.cpp /opt/llama.cpp

# libxext6 for libGLX_nvidia.so.0
# libegl1 for nvidia vulkan icd
# llama.cpp built with libcurl
RUN --mount=type=cache,target=/var/cache/apt \
    apt-get update && \
    apt-get install -y libvulkan1 libgomp1 curl vulkan-tools libegl1 libxext6 && \
    rm -rf /var/lib/apt/lists/*

ENV NVIDIA_DRIVER_CAPABILITIES=all

ENTRYPOINT []
